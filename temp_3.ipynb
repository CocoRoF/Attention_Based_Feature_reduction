{"cells":[{"cell_type":"code","execution_count":130,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1694584860067,"user":{"displayName":"HR Jang","userId":"13830645666148558230"},"user_tz":-540},"id":"B5i89UMJS8sv"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from util.module import *\n","from util.utility import *\n","import lightgbm as lgb\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","df = pd.read_csv('./Data/house-prices-advanced-regression-techniques/train.csv')\n","numeric_df = df.select_dtypes(include=['int', 'float'])\n","\n","numeric_df = numeric_df.dropna(axis=0)\n","numeric_df.reset_index(inplace=True, drop=True)\n","test_df_x = numeric_df.iloc[:, 1:23]\n","test_df_y = numeric_df.iloc[:, -1]\n","\n","array_df = np.array(test_df_x)\n","fa = Factor_attention(array_df, dim_information=512)\n","fa.col_to_vec(threshold = 0.35)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":132,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":629},"executionInfo":{"elapsed":706,"status":"error","timestamp":1694585118786,"user":{"displayName":"HR Jang","userId":"13830645666148558230"},"user_tz":-540},"id":"CfnxVaqOIEpK","outputId":"feec3ef8-dfb7-4f0f-964d-75b51ded0ead"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [5/500], Loss: 40.3287\n","Epoch [10/500], Loss: 39.4541\n","Epoch [15/500], Loss: 39.2490\n","Epoch [20/500], Loss: 38.7597\n","Epoch [25/500], Loss: 38.4184\n","Epoch [30/500], Loss: 37.9309\n","Epoch [35/500], Loss: 37.8154\n","Epoch [40/500], Loss: 37.7897\n","Epoch [45/500], Loss: 38.9754\n","Epoch [50/500], Loss: 36.1844\n","Epoch [55/500], Loss: 36.7796\n","Epoch [60/500], Loss: 36.8179\n","Epoch [65/500], Loss: 35.4754\n","Epoch [70/500], Loss: 36.2901\n","Epoch [75/500], Loss: 36.6105\n","Epoch [80/500], Loss: 36.5661\n","Epoch [85/500], Loss: 36.3562\n","Epoch [90/500], Loss: 36.2962\n","Epoch [95/500], Loss: 36.2256\n","Epoch [100/500], Loss: 43.3371\n","Epoch [105/500], Loss: 43.1581\n","Epoch [110/500], Loss: 42.5281\n","Epoch [115/500], Loss: 42.0577\n","Epoch [120/500], Loss: 41.8639\n","Epoch [125/500], Loss: 41.5331\n","Epoch [130/500], Loss: 40.9311\n","Epoch [135/500], Loss: 39.8659\n","Epoch [140/500], Loss: 38.6623\n","Epoch [145/500], Loss: 38.0632\n","Epoch [150/500], Loss: 37.7817\n","Epoch [155/500], Loss: 37.6914\n","Epoch [160/500], Loss: 37.5423\n","Epoch [165/500], Loss: 37.2168\n","Epoch [170/500], Loss: 36.6771\n","Epoch [175/500], Loss: 33.4043\n","Epoch [180/500], Loss: 37.1475\n","Epoch [185/500], Loss: 43.8607\n","Epoch [190/500], Loss: 44.6839\n","Epoch [195/500], Loss: 45.0832\n","Epoch [200/500], Loss: 45.1479\n","Epoch [205/500], Loss: 45.0599\n","Epoch [210/500], Loss: 44.8985\n","Epoch [215/500], Loss: 44.7024\n","Epoch [220/500], Loss: 44.4914\n","Epoch [225/500], Loss: 44.2750\n","Epoch [230/500], Loss: 44.0578\n","Epoch [235/500], Loss: 43.8411\n","Epoch [240/500], Loss: 43.6246\n","Epoch [245/500], Loss: 43.4058\n","Epoch [250/500], Loss: 43.1558\n","Epoch [255/500], Loss: 42.9330\n","Epoch [260/500], Loss: 42.7371\n","Epoch [265/500], Loss: 42.5374\n","Epoch [270/500], Loss: 42.3294\n","Epoch [275/500], Loss: 42.1116\n","Epoch [280/500], Loss: 41.9189\n","Epoch [285/500], Loss: 41.7115\n","Epoch [290/500], Loss: 41.5012\n","Epoch [295/500], Loss: 41.2714\n","Epoch [300/500], Loss: 40.9879\n","Epoch [305/500], Loss: 40.6115\n","Epoch [310/500], Loss: 40.0728\n","Epoch [315/500], Loss: 39.3719\n","Epoch [320/500], Loss: 38.1942\n","Epoch [325/500], Loss: 37.3182\n","Epoch [330/500], Loss: 37.1288\n","Epoch [335/500], Loss: 36.8976\n","Epoch [340/500], Loss: 36.4309\n","Epoch [345/500], Loss: 36.0704\n","Epoch [350/500], Loss: 35.8553\n","Epoch [355/500], Loss: 35.5682\n","Epoch [360/500], Loss: 35.2314\n","Epoch [365/500], Loss: 34.8202\n","Epoch [370/500], Loss: 34.2542\n","Epoch [375/500], Loss: 33.6466\n","Epoch [380/500], Loss: 33.4997\n","Epoch [385/500], Loss: 33.6034\n","Epoch [390/500], Loss: 33.3776\n","Epoch [395/500], Loss: 33.4038\n","Epoch [400/500], Loss: 33.3872\n","Epoch [405/500], Loss: 33.3596\n","Epoch [410/500], Loss: 33.3287\n","Epoch [415/500], Loss: 33.3021\n","Epoch [420/500], Loss: 33.2835\n","Epoch [425/500], Loss: 33.2662\n","Epoch [430/500], Loss: 33.2566\n","Epoch [435/500], Loss: 33.2388\n","Epoch [440/500], Loss: 33.2258\n","Epoch [445/500], Loss: 33.2022\n","Epoch [450/500], Loss: 33.1969\n","Epoch [455/500], Loss: 33.1775\n","Epoch [460/500], Loss: 33.1540\n","Epoch [465/500], Loss: 33.1388\n","Epoch [470/500], Loss: 33.1201\n","Epoch [475/500], Loss: 33.1066\n","Epoch [480/500], Loss: 33.0830\n","Epoch [485/500], Loss: 33.0661\n","Epoch [490/500], Loss: 33.0460\n","Epoch [495/500], Loss: 33.0258\n","Epoch [500/500], Loss: 33.0096\n"]}],"source":["model = Attention(n_factor= fa.n_factors, info_dim = fa.dim_info)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n","\n","num_epochs = 500\n","for epoch in range(num_epochs):\n","    reconstructed = model(fa)\n","    loss = pearson_correlation_coefficient_loss_function(reconstructed)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    torch.autograd.set_detect_anomaly(True)\n","\n","    if (epoch+1) % 5 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1121, 12])"]},"execution_count":133,"metadata":{},"output_type":"execute_result"}],"source":["model.total_result.shape\n"]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[],"source":["# 임의의 데이터 생성\n","# X = np.array(numeric_df.iloc[:, 1:23])\n","X = torch.Tensor.cpu(model.total_result).detach().numpy()\n","Y = np.array(numeric_df.iloc[:, -1])  # 임의의 레이블 생성\n","\n","# 데이터 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","# Min-Max 정규화\n","scaler_X = MinMaxScaler()\n","X_train = scaler_X.fit_transform(X_train)\n","X_test = scaler_X.transform(X_test)\n","\n","scaler_Y = MinMaxScaler()\n","y_train = scaler_Y.fit_transform(y_train.reshape(-1, 1)).ravel()\n","y_test = scaler_Y.transform(y_test.reshape(-1, 1)).ravel()\n"]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[{"data":{"text/plain":["185506.15254237287"]},"execution_count":128,"metadata":{},"output_type":"execute_result"}],"source":["Y.mean()"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 567 candidates, totalling 1701 fits\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000452 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 2061\n","[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 11\n","[LightGBM] [Info] Start training from score 0.208582\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Best parameters found:  {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 100, 'subsample': 0.5}\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","RMSE with best parameters: 45422.69775548203\n"]}],"source":["# LightGBM 회귀 모델 객체 생성\n","lgb_model = lgb.LGBMRegressor()\n","\n","# 탐색할 파라미터 설정\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'max_depth': [10,13,15,17,20,25,30],\n","    'colsample_bytree': [0.5, 0.7, 1.0],\n","    'subsample': [0.5, 0.7, 1.0]\n","}\n","\n","# GridSearchCV 객체 생성\n","grid_search = GridSearchCV(lgb_model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n","grid_search.fit(X_train, y_train)\n","\n","# 최적의 파라미터 출력\n","print(\"Best parameters found: \", grid_search.best_params_)\n","\n","# 최적의 파라미터로 모델 훈련\n","best_lgb_model = grid_search.best_estimator_\n","\n","# 예측 수행\n","y_pred = best_lgb_model.predict(X_test)\n","\n","# 예측값을 원래의 스케일로 변환\n","y_pred_original = scaler_Y.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n","\n","# 원래 스케일의 y_test 값도 변환\n","y_test_original = scaler_Y.inverse_transform(y_test.reshape(-1, 1)).ravel()\n","\n","# RMSE 계산 및 출력\n","rmse = mean_squared_error(y_test_original, y_pred_original, squared=False)\n","print(f\"RMSE with best parameters: {rmse}\")"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 567 candidates, totalling 1701 fits\n","Best parameters found:  {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 100, 'subsample': 0.5}\n","RMSE with best parameters: 47120.18161661042\n"]}],"source":["# XGBoost 회귀 모델 객체 생성\n","xgb_model = xgb.XGBRegressor()\n","\n","# 탐색할 파라미터 설정\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'max_depth': [10,13,15,17,20,25,30],\n","    'colsample_bytree': [0.5, 0.7, 1.0],\n","    'subsample': [0.5, 0.7, 1.0]\n","}\n","\n","# GridSearchCV 객체 생성\n","grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n","grid_search.fit(X_train, y_train)\n","\n","# 최적의 파라미터 출력\n","print(\"Best parameters found: \", grid_search.best_params_)\n","\n","# 최적의 파라미터로 모델 훈련\n","best_xgb_model = grid_search.best_estimator_\n","\n","# 예측 수행\n","y_pred = best_xgb_model.predict(X_test)\n","\n","# 예측값을 원래의 스케일로 변환\n","y_pred_original = scaler_Y.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n","\n","# 원래 스케일의 y_test 값도 변환\n","y_test_original = scaler_Y.inverse_transform(y_test.reshape(-1, 1)).ravel()\n","\n","# RMSE 계산 및 출력\n","rmse = mean_squared_error(y_test_original, y_pred_original, squared=False)\n","print(f\"RMSE with best parameters: {rmse}\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
