{"cells":[{"cell_type":"code","execution_count":119,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1694584860067,"user":{"displayName":"HR Jang","userId":"13830645666148558230"},"user_tz":-540},"id":"B5i89UMJS8sv"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from util.module import *\n","from util.utility import *\n","import lightgbm as lgb\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","df = pd.read_csv('./Data/house-prices-advanced-regression-techniques/train.csv')\n","numeric_df = df.select_dtypes(include=['int', 'float'])\n","\n","numeric_df = numeric_df.dropna(axis=0)\n","numeric_df.reset_index(inplace=True, drop=True)\n","test_df_x = numeric_df.iloc[:, 1:23]\n","test_df_y = numeric_df.iloc[:, -1]\n","\n","array_df = np.array(test_df_x)\n","fa = Factor_attention(array_df)\n","fa.col_to_vec(threshold = 0.35)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":120,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":629},"executionInfo":{"elapsed":706,"status":"error","timestamp":1694585118786,"user":{"displayName":"HR Jang","userId":"13830645666148558230"},"user_tz":-540},"id":"CfnxVaqOIEpK","outputId":"feec3ef8-dfb7-4f0f-964d-75b51ded0ead"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [5/500], Loss: 35.1009\n","Epoch [10/500], Loss: 42.3433\n","Epoch [15/500], Loss: 39.1725\n","Epoch [20/500], Loss: 37.6794\n","Epoch [25/500], Loss: 36.4373\n","Epoch [30/500], Loss: 35.4437\n","Epoch [35/500], Loss: 34.6289\n","Epoch [40/500], Loss: 34.2486\n","Epoch [45/500], Loss: 35.3511\n","Epoch [50/500], Loss: 35.4140\n","Epoch [55/500], Loss: 34.7932\n","Epoch [60/500], Loss: 33.9506\n","Epoch [65/500], Loss: 36.1903\n","Epoch [70/500], Loss: 35.8456\n","Epoch [75/500], Loss: 33.8609\n","Epoch [80/500], Loss: 34.3106\n","Epoch [85/500], Loss: 33.6806\n","Epoch [90/500], Loss: 33.7153\n","Epoch [95/500], Loss: 33.7242\n","Epoch [100/500], Loss: 34.6471\n","Epoch [105/500], Loss: 33.9718\n","Epoch [110/500], Loss: 33.5875\n","Epoch [115/500], Loss: 33.7005\n","Epoch [120/500], Loss: 33.5594\n","Epoch [125/500], Loss: 33.5203\n","Epoch [130/500], Loss: 33.4525\n","Epoch [135/500], Loss: 33.2413\n","Epoch [140/500], Loss: 32.5767\n","Epoch [145/500], Loss: 32.1069\n","Epoch [150/500], Loss: 31.7273\n","Epoch [155/500], Loss: 31.5004\n","Epoch [160/500], Loss: 31.0869\n","Epoch [165/500], Loss: 30.9641\n","Epoch [170/500], Loss: 30.6807\n","Epoch [175/500], Loss: 30.5482\n","Epoch [180/500], Loss: 30.4320\n","Epoch [185/500], Loss: 30.3507\n","Epoch [190/500], Loss: 30.1253\n","Epoch [195/500], Loss: 30.1361\n","Epoch [200/500], Loss: 29.7839\n","Epoch [205/500], Loss: 29.5128\n","Epoch [210/500], Loss: 29.2624\n","Epoch [215/500], Loss: 29.0390\n","Epoch [220/500], Loss: 28.8138\n","Epoch [225/500], Loss: 28.5765\n","Epoch [230/500], Loss: 29.1277\n","Epoch [235/500], Loss: 28.6483\n","Epoch [240/500], Loss: 28.9594\n","Epoch [245/500], Loss: 28.5857\n","Epoch [250/500], Loss: 28.2621\n","Epoch [255/500], Loss: 28.5247\n","Epoch [260/500], Loss: 28.3510\n","Epoch [265/500], Loss: 28.2901\n","Epoch [270/500], Loss: 28.5584\n","Epoch [275/500], Loss: 28.3736\n","Epoch [280/500], Loss: 28.3234\n","Epoch [285/500], Loss: 28.6029\n","Epoch [290/500], Loss: 28.6351\n","Epoch [295/500], Loss: 28.1624\n","Epoch [300/500], Loss: 28.1204\n","Epoch [305/500], Loss: 28.0302\n","Epoch [310/500], Loss: 27.8068\n","Epoch [315/500], Loss: 27.2835\n","Epoch [320/500], Loss: 30.2319\n","Epoch [325/500], Loss: 28.5169\n","Epoch [330/500], Loss: 26.5312\n","Epoch [335/500], Loss: 26.0001\n","Epoch [340/500], Loss: 26.1951\n","Epoch [345/500], Loss: 25.8037\n","Epoch [350/500], Loss: 25.8364\n","Epoch [355/500], Loss: 25.8937\n","Epoch [360/500], Loss: 25.5704\n","Epoch [365/500], Loss: 25.5529\n","Epoch [370/500], Loss: 25.2959\n","Epoch [375/500], Loss: 25.5381\n","Epoch [380/500], Loss: 25.2353\n","Epoch [385/500], Loss: 25.4031\n","Epoch [390/500], Loss: 25.3198\n","Epoch [395/500], Loss: 25.8196\n","Epoch [400/500], Loss: 25.7933\n","Epoch [405/500], Loss: 25.4602\n","Epoch [410/500], Loss: 25.1373\n","Epoch [415/500], Loss: 25.2591\n","Epoch [420/500], Loss: 25.5023\n","Epoch [425/500], Loss: 25.0931\n","Epoch [430/500], Loss: 25.2188\n","Epoch [435/500], Loss: 25.5349\n","Epoch [440/500], Loss: 25.6933\n","Epoch [445/500], Loss: 25.4607\n","Epoch [450/500], Loss: 25.6543\n","Epoch [455/500], Loss: 25.5357\n","Epoch [460/500], Loss: 24.9866\n","Epoch [465/500], Loss: 25.0046\n","Epoch [470/500], Loss: 24.9345\n","Epoch [475/500], Loss: 25.0517\n","Epoch [480/500], Loss: 24.7975\n","Epoch [485/500], Loss: 24.9928\n","Epoch [490/500], Loss: 24.7655\n","Epoch [495/500], Loss: 24.8140\n","Epoch [500/500], Loss: 24.7339\n"]}],"source":["model = Attention(n_factor= fa.n_factors, info_dim = fa.dim_info)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.035)\n","\n","num_epochs = 500\n","for epoch in range(num_epochs):\n","    reconstructed = model(fa)\n","    loss = pearson_correlation_coefficient_loss_function(reconstructed)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    torch.autograd.set_detect_anomaly(True)\n","\n","    if (epoch+1) % 5 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1121, 12])"]},"execution_count":121,"metadata":{},"output_type":"execute_result"}],"source":["model.total_result.shape\n"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["# 임의의 데이터 생성\n","# X = np.array(numeric_df.iloc[:, 1:23])\n","X = torch.Tensor.cpu(model.total_result).detach().numpy()\n","Y = np.array(numeric_df.iloc[:, -1])  # 임의의 레이블 생성\n","\n","# 데이터 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","# Min-Max 정규화\n","scaler_X = MinMaxScaler()\n","X_train = scaler_X.fit_transform(X_train)\n","X_test = scaler_X.transform(X_test)\n","\n","scaler_Y = MinMaxScaler()\n","y_train = scaler_Y.fit_transform(y_train.reshape(-1, 1)).ravel()\n","y_test = scaler_Y.transform(y_test.reshape(-1, 1)).ravel()\n"]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[{"data":{"text/plain":["185506.15254237287"]},"execution_count":128,"metadata":{},"output_type":"execute_result"}],"source":["Y.mean()"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 567 candidates, totalling 1701 fits\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000189 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 2070\n","[LightGBM] [Info] Number of data points in the train set: 896, number of used features: 11\n","[LightGBM] [Info] Start training from score 0.208582\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Best parameters found:  {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 13, 'n_estimators': 100, 'subsample': 0.5}\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","RMSE with best parameters: 45198.330806550206\n"]}],"source":["# LightGBM 회귀 모델 객체 생성\n","lgb_model = lgb.LGBMRegressor()\n","\n","# 탐색할 파라미터 설정\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'max_depth': [10,13,15,17,20,25,30],\n","    'colsample_bytree': [0.5, 0.7, 1.0],\n","    'subsample': [0.5, 0.7, 1.0]\n","}\n","\n","# GridSearchCV 객체 생성\n","grid_search = GridSearchCV(lgb_model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n","grid_search.fit(X_train, y_train)\n","\n","# 최적의 파라미터 출력\n","print(\"Best parameters found: \", grid_search.best_params_)\n","\n","# 최적의 파라미터로 모델 훈련\n","best_lgb_model = grid_search.best_estimator_\n","\n","# 예측 수행\n","y_pred = best_lgb_model.predict(X_test)\n","\n","# 예측값을 원래의 스케일로 변환\n","y_pred_original = scaler_Y.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n","\n","# 원래 스케일의 y_test 값도 변환\n","y_test_original = scaler_Y.inverse_transform(y_test.reshape(-1, 1)).ravel()\n","\n","# RMSE 계산 및 출력\n","rmse = mean_squared_error(y_test_original, y_pred_original, squared=False)\n","print(f\"RMSE with best parameters: {rmse}\")"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 567 candidates, totalling 1701 fits\n","Best parameters found:  {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 15, 'n_estimators': 100, 'subsample': 0.5}\n","RMSE with best parameters: 47120.18161661042\n"]}],"source":["# XGBoost 회귀 모델 객체 생성\n","xgb_model = xgb.XGBRegressor()\n","\n","# 탐색할 파라미터 설정\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'max_depth': [10,13,15,17,20,25,30],\n","    'colsample_bytree': [0.5, 0.7, 1.0],\n","    'subsample': [0.5, 0.7, 1.0]\n","}\n","\n","# GridSearchCV 객체 생성\n","grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n","grid_search.fit(X_train, y_train)\n","\n","# 최적의 파라미터 출력\n","print(\"Best parameters found: \", grid_search.best_params_)\n","\n","# 최적의 파라미터로 모델 훈련\n","best_xgb_model = grid_search.best_estimator_\n","\n","# 예측 수행\n","y_pred = best_xgb_model.predict(X_test)\n","\n","# 예측값을 원래의 스케일로 변환\n","y_pred_original = scaler_Y.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n","\n","# 원래 스케일의 y_test 값도 변환\n","y_test_original = scaler_Y.inverse_transform(y_test.reshape(-1, 1)).ravel()\n","\n","# RMSE 계산 및 출력\n","rmse = mean_squared_error(y_test_original, y_pred_original, squared=False)\n","print(f\"RMSE with best parameters: {rmse}\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
